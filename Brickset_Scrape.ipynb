{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a4ca2b8-fd5e-4b57-8ce9-3c176c6d407a",
   "metadata": {},
   "source": [
    "# Web Scraper Syacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa723f7a-d272-4c5f-b3e5-865b83b8f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Function for a single page\n",
    "def scrape_page(driver):\n",
    "    sets = driver.find_elements(By.CSS_SELECTOR, \"article.set\")  \n",
    "    data = []\n",
    "    \n",
    "    for set_ in sets:\n",
    "        try:\n",
    "            # Extract 'Id'\n",
    "            id_element = set_.find_element(By.XPATH, \".//div[@class='meta']//a[contains(@href, '/sets/')]\")\n",
    "            lego_id = id_element.text.strip()\n",
    "\n",
    "            # Extract 'Name'\n",
    "            name_element = set_.find_element(By.XPATH, \".//h1/a\")\n",
    "            lego_name = name_element.text.strip()\n",
    "\n",
    "            # Extract 'Pieces'\n",
    "            pieces_element = set_.find_element(By.XPATH, \".//dt[normalize-space()='Pieces']/following-sibling::dd\")\n",
    "            pieces = pieces_element.text.strip()\n",
    "\n",
    "            # Extract 'Minifigs'\n",
    "            minifigs_element = set_.find_element(By.XPATH, \".//dt[normalize-space()='Minifigs']/following-sibling::dd\")\n",
    "            minifigs = minifigs_element.text.strip()\n",
    "\n",
    "            # Extract 'RRP'\n",
    "            rrp_element = set_.find_element(By.XPATH, \".//dt[normalize-space()='RRP']/following-sibling::dd\")\n",
    "            rrp = rrp_element.text.strip().split(\"|\")[0]  # takes the first RRP\n",
    "\n",
    "            data.append({\n",
    "                \"id\": lego_id,\n",
    "                \"SetName\": lego_name,\n",
    "                \"Pieces\": pieces,\n",
    "                \"Minifigs\": minifigs,\n",
    "                \"USRetailPrice\": rrp\n",
    "            })\n",
    "        except NoSuchElementException:\n",
    "            print(\"Some elements are missing on this page.\")\n",
    "            continue\n",
    "\n",
    "    return data\n",
    "\n",
    "# Main scraping function\n",
    "def scrape_lego_sets(base_url, max_pages=12000):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  \n",
    "    service = Service(\"path/to/chromedriver\")  \n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    all_data = []\n",
    "    current_page = 1\n",
    "\n",
    "    try:\n",
    "        while current_page <= max_pages:\n",
    "            print(f\"Scraping page {current_page}...\")\n",
    "            url = f\"{base_url}/page-{current_page}\"\n",
    "            driver.get(url)\n",
    "            time.sleep(2)  # Wait for the page to load\n",
    "            \n",
    "            # Scrape current page\n",
    "            page_data = scrape_page(driver)\n",
    "            all_data.extend(page_data)\n",
    "\n",
    "            # check for next\n",
    "            try:\n",
    "                next_button = driver.find_element(By.LINK_TEXT, str(current_page + 1))\n",
    "                current_page += 1\n",
    "            except NoSuchElementException:\n",
    "                print(\"No more pages found.\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df\n",
    "\n",
    "# Set base URL\n",
    "BASE_URL = \"https://brickset.com/sets/query-9196\"\n",
    "\n",
    "# Run the scraper\n",
    "lego_df = scrape_lego_sets(BASE_URL, max_pages=12000) \n",
    "\n",
    "# Save the results\n",
    "lego_df.to_csv(\"Brickset-list.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e8484-a071-41f1-8f91-27006e1f9b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
